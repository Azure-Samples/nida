{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# add path so we can find files under src/services\n",
    "import sys\n",
    "sys.path.append('../src/services')\n",
    "from src.services import azure_oai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract the \"display\" field\n",
    "def extract_display_field(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Navigate to \"combinedRecognizedPhrases\"\n",
    "        phrases = data.get('combinedRecognizedPhrases', [])\n",
    "        \n",
    "        if not phrases:\n",
    "            print(\"No 'combinedRecognizedPhrases' found in the JSON data.\")\n",
    "            return None\n",
    "        \n",
    "        # Assuming you want the \"display\" field from the first item\n",
    "        first_phrase = phrases[0]\n",
    "        display_content = first_phrase.get('display')\n",
    "        \n",
    "        if display_content:\n",
    "            return display_content\n",
    "        else:\n",
    "            print(\"'display' field not found in the first 'combinedRecognizedPhrases' item.\")\n",
    "            return None\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through all text file transcripts in data/transcripts\n",
    "#and calling azure_oai.call_llm() on each one\n",
    "\n",
    "prompt_file = \"./samples/marketing_sentiment_details.txt\"\n",
    "save_transcripts = False\n",
    "transcripts_folder = \"transcripts_whisper_o4\"\n",
    "\n",
    "pure_text = transcripts_folder == \"transcripts_whisper_o4\"\n",
    "\n",
    "for filename in os.listdir(f'data/{transcripts_folder}'):\n",
    "    if pure_text:\n",
    "        with open(f'data/{transcripts_folder}/{filename}', 'r') as file:\n",
    "            transcript = file.read()\n",
    "    else:\n",
    "        if filename.endswith('.json'):\n",
    "            transcript = extract_display_field(f'data/{transcripts_folder}/{filename}')\n",
    "            # save transcript to text file for the streamlit app\n",
    "            if save_transcripts:\n",
    "                with open(f'data/{transcripts_folder}/{filename.split(\".\")[0]}.txt', 'w') as file:\n",
    "                    file.write(transcript)\n",
    "    #print(transcript)\n",
    "    result = azure_oai.call_llm(prompt_file, transcript)\n",
    "    name = filename.split('.')[0]\n",
    "    with open(f'data/llm_analysis/{name}.json', 'w') as file:\n",
    "        file.write(result)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all results in one CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(json_data):\n",
    "    \"\"\"\n",
    "    Flatten the nested JSON structure into a single-level dictionary.\n",
    "    The keys will be in the format 'Parameter X - Metric Name'.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    for param, metrics in json_data.items():\n",
    "        for metric, value in metrics.items():\n",
    "            # Create a unique key for each metric\n",
    "            flat_key = f\"{param} - {metric}\"\n",
    "            flat_dict[flat_key] = value\n",
    "    return flat_dict\n",
    "\n",
    "\n",
    "def json_files_to_csv(json_directory, output_csv):\n",
    "    all_calls = []\n",
    "    \n",
    "    # Use pathlib for better path handling\n",
    "    json_dir = Path(json_directory)\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not json_dir.exists() or not json_dir.is_dir():\n",
    "        raise ValueError(f\"The directory {json_directory} does not exist or is not a directory.\")\n",
    "    \n",
    "    # Iterate over all JSON files in the directory\n",
    "    for json_file in json_dir.glob(\"*.json\"):\n",
    "        with open(json_file, 'r', encoding='utf-8') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from file {json_file.name}: {e}\")\n",
    "                continue  # Skip files with invalid JSON\n",
    "            \n",
    "            # Flatten the JSON structure\n",
    "            flat_data = flatten_json(data)\n",
    "            \n",
    "            # Add the call identifier (file name without extension)\n",
    "            call_id = json_file.stem\n",
    "            flat_data['Call ID'] = call_id\n",
    "            \n",
    "            \n",
    "            # Append the processed data to the list\n",
    "            all_calls.append(flat_data)\n",
    "    \n",
    "    if not all_calls:\n",
    "        print(\"No valid JSON files found or all files are invalid.\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(all_calls)\n",
    "    # Reorder columns to have 'Call ID' as the first column\n",
    "    columns = ['Call ID'] + [col for col in df.columns if col != 'Call ID']\n",
    "    df = df[columns]\n",
    "    \n",
    "    \n",
    "    # Optionally, represent the average as a percentage\n",
    "    # df['Average YES (%)'] = df['Average YES'] * 100\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Combined CSV with summaries saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_directory = \"./data/llm_analysis/\"  \n",
    "output_csv = \"./data/combined_calls.csv\"  \n",
    "    \n",
    "json_files_to_csv(json_directory, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the results with the ground truth is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_with_ground_truth(combined_csv_path, ground_truth_excel_path, output_csv_path, drop_unmatched=False):\n",
    "    \"\"\"\n",
    "    Merges the combined CSV with the ground truth data from the Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    - combined_csv_path: Path to the combined CSV file generated from JSON files.\n",
    "    - ground_truth_excel_path: Path to the ground truth Excel file.\n",
    "    - output_csv_path: Path where the merged CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Check if the combined CSV exists\n",
    "    if not Path(combined_csv_path).is_file():\n",
    "        raise FileNotFoundError(f\"The combined CSV file {combined_csv_path} does not exist.\")\n",
    "    \n",
    "    # Check if the Excel file exists\n",
    "    if not Path(ground_truth_excel_path).is_file():\n",
    "        raise FileNotFoundError(f\"The Excel file {ground_truth_excel_path} does not exist.\")\n",
    "    \n",
    "    # Read the combined CSV\n",
    "    try:\n",
    "        combined_df = pd.read_csv(combined_csv_path)\n",
    "        print(f\"Loaded combined CSV with {len(combined_df)} records.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading combined CSV: {e}\")\n",
    "    \n",
    "    # Read the ground truth Excel file\n",
    "    try:\n",
    "        ground_truth_df = pd.read_excel(ground_truth_excel_path, sheet_name='Parameters')\n",
    "        print(f\"Loaded ground truth Excel with {len(ground_truth_df)} records.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading Excel file: {e}\")\n",
    "    \n",
    "    # Inspect column names\n",
    "    required_columns = ['Call ID',\t'Parameter 1',\t'Parameter 2',\t'Parameter 3']\n",
    "    for col in required_columns:\n",
    "        if col not in ground_truth_df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the 'Parameter' sheet of the Excel file.\")\n",
    "    \n",
    "    # Filter out rows where 'AUDIO FILE NAME' is not found in ground truth\n",
    "    ground_truth_df = ground_truth_df[ground_truth_df['Call ID'].notna()]\n",
    "    ground_truth_df['Call ID'] = ground_truth_df['Call ID'].astype(str)\n",
    "    # drop column MSDIN\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['MSDIN'])\n",
    "\n",
    "    # Convert 'Call ID' columns in both DataFrames to string to ensure consistency\n",
    "    combined_df['Call ID'] = combined_df['Call ID'].astype(str)\n",
    "    \n",
    "    # Check for duplicates in ground truth\n",
    "    if ground_truth_df['Call ID'].duplicated().any():\n",
    "        duplicates = ground_truth_df[ground_truth_df['Call ID'].duplicated(keep=False)]\n",
    "        print(duplicates)\n",
    "        print(\"Warning: Duplicate Call IDs found in ground truth data. Using the first occurrence.\")\n",
    "        ground_truth_df = ground_truth_df.drop_duplicates(subset=['Call ID'], keep='first')\n",
    "    \n",
    "   \n",
    "    # Merge the combined CSV with ground truth\n",
    "    merged_df = pd.merge(combined_df, ground_truth_df, on='Call ID', how='left')\n",
    "    \n",
    "    # Check for any Call IDs that didn't find a match\n",
    "    unmatched = merged_df[merged_df['Parameter 1'].isna()]\n",
    "    if not unmatched.empty:\n",
    "        print(f\"Warning: {len(unmatched)} Call IDs did not find a matching Parameter X in the ground truth data.\")\n",
    "    \n",
    "     # If drop_unmatched is True, remove rows that didn't match ground truth\n",
    "    if drop_unmatched:\n",
    "        original_count = len(merged_df)\n",
    "        merged_df = merged_df[merged_df['Parameter 1'].notna()]\n",
    "        print(f\"Dropped {original_count - len(merged_df)} unmatched rows based on ground truth data.\")\n",
    "\n",
    "    try:\n",
    "        merged_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Merged CSV has been saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error saving merged CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_path = \"./data/combined_calls.csv\"  # Path to your combined CSV from previous step\n",
    "ground_truth_excel_path = \"./data/sales_call_quality_evaluation.xlsx\"  # Path to your ground truth Excel file\n",
    "output_csv_path = \"./data/merged_calls_with_scores.csv\"  # Desired output path for the merged CSV\n",
    "    \n",
    "# Perform the merge\n",
    "merge_csv_with_ground_truth(combined_csv_path, ground_truth_excel_path, output_csv_path, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
